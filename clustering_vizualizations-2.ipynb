{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nets import GraphNN_KNN_v1, EdgeClassifier_v1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, accuracy_score, average_precision_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from preprocessing import preprocess_dataset\n",
    "from clustering_metrics import class_disbalance_graphx, class_disbalance_graphx__\n",
    "from clustering_metrics import estimate_e, estimate_start_xyz, estimate_txty\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def predict_one_shower(shower, graph_embedder, edge_classifier):\n",
    "    embeddings = graph_embedder(shower)\n",
    "    edge_labels_true = (shower.y[shower.edge_index[0]] == shower.y[shower.edge_index[1]]).view(-1)\n",
    "    edge_data = torch.cat([\n",
    "        embeddings[shower.edge_index[0]],\n",
    "        embeddings[shower.edge_index[1]]\n",
    "    ], dim=1)\n",
    "    edge_labels_predicted = edge_classifier(edge_data).view(-1)\n",
    "\n",
    "    return edge_labels_true, edge_labels_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall torch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall torch-geometric -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from hdbscan import plots as hdbscan_plot\n",
    "import networkx as nx\n",
    "from clustering import preprocess_torch_shower_to_nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_cl = 40\n",
    "cl_size = min_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafile='clusters_rand.pt'\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterized_bricks = torch.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_disbalance_graphx(graphx):\n",
    "    signal = []\n",
    "    for _, node in graphx.nodes(data=True):\n",
    "        signal.append(node['signal'])\n",
    "    return list(zip(*np.unique(signal, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "N predicted showers per brick: 181\n",
      "109\n",
      "N predicted showers per brick: 71\n",
      "132\n",
      "N predicted showers per brick: 95\n",
      "196\n",
      "N predicted showers per brick: 143\n",
      "162\n",
      "N predicted showers per brick: 123\n",
      "279\n",
      "N predicted showers per brick: 196\n",
      "138\n",
      "N predicted showers per brick: 93\n",
      "153\n",
      "N predicted showers per brick: 120\n",
      "226\n",
      "N predicted showers per brick: 155\n",
      "230\n",
      "N predicted showers per brick: 176\n",
      "189\n",
      "N predicted showers per brick: 141\n",
      "223\n",
      "N predicted showers per brick: 171\n",
      "192\n",
      "N predicted showers per brick: 137\n",
      "223\n",
      "N predicted showers per brick: 164\n",
      "265\n",
      "N predicted showers per brick: 195\n",
      "74\n",
      "N predicted showers per brick: 58\n",
      "234\n",
      "N predicted showers per brick: 182\n",
      "155\n",
      "N predicted showers per brick: 109\n",
      "189\n",
      "N predicted showers per brick: 133\n",
      "156\n",
      "N predicted showers per brick: 114\n",
      "275\n",
      "N predicted showers per brick: 180\n",
      "187\n",
      "N predicted showers per brick: 132\n",
      "175\n",
      "N predicted showers per brick: 116\n",
      "224\n",
      "N predicted showers per brick: 152\n",
      "66\n",
      "N predicted showers per brick: 57\n",
      "158\n",
      "N predicted showers per brick: 115\n",
      "122\n",
      "N predicted showers per brick: 85\n",
      "114\n",
      "N predicted showers per brick: 84\n",
      "88\n",
      "N predicted showers per brick: 64\n",
      "87\n",
      "N predicted showers per brick: 58\n",
      "272\n",
      "N predicted showers per brick: 195\n",
      "243\n",
      "N predicted showers per brick: 169\n",
      "293\n",
      "N predicted showers per brick: 212\n",
      "236\n",
      "N predicted showers per brick: 167\n",
      "246\n",
      "N predicted showers per brick: 155\n",
      "209\n",
      "N predicted showers per brick: 144\n",
      "61\n",
      "N predicted showers per brick: 45\n",
      "96\n",
      "N predicted showers per brick: 72\n",
      "248\n",
      "N predicted showers per brick: 179\n",
      "240\n",
      "N predicted showers per brick: 173\n",
      "191\n",
      "N predicted showers per brick: 141\n",
      "156\n",
      "N predicted showers per brick: 118\n",
      "146\n",
      "N predicted showers per brick: 108\n",
      "56\n",
      "N predicted showers per brick: 44\n",
      "79\n",
      "N predicted showers per brick: 63\n",
      "90\n",
      "N predicted showers per brick: 63\n",
      "260\n",
      "N predicted showers per brick: 193\n",
      "255\n",
      "N predicted showers per brick: 183\n",
      "144\n",
      "N predicted showers per brick: 95\n",
      "161\n",
      "N predicted showers per brick: 118\n",
      "75\n",
      "N predicted showers per brick: 57\n",
      "50\n",
      "N predicted showers per brick: 39\n",
      "289\n",
      "N predicted showers per brick: 183\n",
      "299\n",
      "N predicted showers per brick: 214\n",
      "130\n",
      "N predicted showers per brick: 92\n",
      "56\n",
      "N predicted showers per brick: 41\n",
      "78\n",
      "N predicted showers per brick: 59\n",
      "94\n",
      "N predicted showers per brick: 69\n",
      "69\n",
      "N predicted showers per brick: 47\n",
      "98\n",
      "N predicted showers per brick: 82\n",
      "108\n",
      "N predicted showers per brick: 83\n",
      "124\n",
      "N predicted showers per brick: 97\n",
      "111\n",
      "N predicted showers per brick: 80\n",
      "105\n",
      "N predicted showers per brick: 84\n",
      "130\n",
      "N predicted showers per brick: 92\n",
      "145\n",
      "N predicted showers per brick: 101\n",
      "167\n",
      "N predicted showers per brick: 135\n",
      "158\n",
      "N predicted showers per brick: 118\n",
      "172\n",
      "N predicted showers per brick: 123\n",
      "183\n",
      "N predicted showers per brick: 134\n",
      "192\n",
      "N predicted showers per brick: 139\n",
      "201\n",
      "N predicted showers per brick: 152\n",
      "210\n",
      "N predicted showers per brick: 155\n",
      "210\n",
      "N predicted showers per brick: 149\n",
      "230\n",
      "N predicted showers per brick: 167\n",
      "222\n",
      "N predicted showers per brick: 156\n",
      "245\n",
      "N predicted showers per brick: 173\n",
      "267\n",
      "N predicted showers per brick: 193\n",
      "287\n",
      "N predicted showers per brick: 213\n",
      "279\n",
      "N predicted showers per brick: 206\n",
      "291\n",
      "N predicted showers per brick: 195\n",
      "300\n",
      "N predicted showers per brick: 227\n",
      "51\n",
      "N predicted showers per brick: 42\n",
      "56\n",
      "N predicted showers per brick: 40\n",
      "67\n",
      "N predicted showers per brick: 51\n",
      "89\n",
      "N predicted showers per brick: 73\n",
      "200\n",
      "N predicted showers per brick: 148\n",
      "143\n",
      "N predicted showers per brick: 104\n",
      "165\n",
      "N predicted showers per brick: 126\n",
      "234\n",
      "N predicted showers per brick: 161\n",
      "289\n",
      "N predicted showers per brick: 199\n",
      "57\n",
      "N predicted showers per brick: 40\n",
      "87\n",
      "N predicted showers per brick: 70\n",
      "62\n",
      "N predicted showers per brick: 48\n",
      "65\n",
      "N predicted showers per brick: 46\n",
      "98\n",
      "N predicted showers per brick: 80\n",
      "100\n",
      "N predicted showers per brick: 77\n",
      "200\n",
      "N predicted showers per brick: 142\n",
      "287\n",
      "N predicted showers per brick: 198\n",
      "76\n",
      "N predicted showers per brick: 52\n",
      "81\n",
      "N predicted showers per brick: 64\n",
      "72\n",
      "N predicted showers per brick: 50\n",
      "93\n",
      "N predicted showers per brick: 64\n",
      "102\n",
      "N predicted showers per brick: 73\n",
      "106\n",
      "N predicted showers per brick: 79\n",
      "298\n",
      "N predicted showers per brick: 217\n",
      "121\n",
      "N predicted showers per brick: 97\n"
     ]
    }
   ],
   "source": [
    "selected_tracks = 0\n",
    "total_tracks = 0\n",
    "E = []\n",
    "E_true_all = []\n",
    "n_showers = []\n",
    "\n",
    "total_number_of_showers = 0\n",
    "number_of_lost_showers = 0\n",
    "second_to_first_ratios = 0\n",
    "number_of_good_showers = 0\n",
    "number_of_stucked_showers = 0\n",
    "number_of_broken_showers = 0\n",
    "\n",
    "second_to_first_ratios = []\n",
    "x_raw = []\n",
    "x_true = []\n",
    "\n",
    "y_raw = []\n",
    "y_true = []\n",
    "\n",
    "z_raw = []\n",
    "z_true = []\n",
    "\n",
    "tx_raw = []\n",
    "tx_true = []\n",
    "\n",
    "ty_raw = []\n",
    "ty_true = []\n",
    "n_showers = []\n",
    "\n",
    "e_stucked_10_list = []\n",
    "e_stucked_30_list = []\n",
    "e_stucked_50_list = []\n",
    "\n",
    "e_broken_10_list = []\n",
    "e_broken_30_list = []\n",
    "e_broken_50_list = []\n",
    "\n",
    "e_good_10_list = []\n",
    "e_good_30_list = []\n",
    "e_good_50_list = []\n",
    "\n",
    "for clusterized_brick in clusterized_bricks:\n",
    "    showers_data = clusterized_brick['graphx'].graph['showers_data']\n",
    "    print(len(showers_data))\n",
    "\n",
    "    clusters = clusterized_brick['clusters']\n",
    "    raw_clusters = clusterized_brick['raw_clusters']\n",
    "    print('N predicted showers per brick:', len(raw_clusters))\n",
    "\n",
    "    for shower_data in showers_data:\n",
    "        shower_data['clusters'] = []\n",
    "        shower_data['raw_clusters'] = []\n",
    "        \n",
    "    for cluster, raw_cluster in zip(clusters, raw_clusters):\n",
    "        selected_tracks += len(cluster)\n",
    "        for label, label_count in class_disbalance_graphx(cluster):\n",
    "            if label_count / showers_data[label]['numtracks'] >= 0.1:\n",
    "                showers_data[label]['clusters'].append(cluster)\n",
    "                showers_data[label]['raw_clusters'].append(raw_cluster)\n",
    "            \n",
    "    for shower_data in showers_data:\n",
    "        total_tracks += shower_data['numtracks']\n",
    "        E.append(shower_data['numtracks'])\n",
    "        E_true_all.append(shower_data['ele_P'])\n",
    "        n_showers.append(len(showers_data)) \n",
    "        \n",
    "    for shower_data in showers_data:\n",
    "            total_number_of_showers += 1\n",
    "\n",
    "            signals_per_cluster = []\n",
    "            signals_per_cluster_bad = []\n",
    "            idx_cluster = []\n",
    "            for i, cluster in enumerate(shower_data['clusters']):\n",
    "                labels, counts = class_disbalance_graphx__(cluster)           \n",
    "                signals_per_cluster.append(counts[labels == shower_data['signal']][0])\n",
    "                idx_cluster.append(i)\n",
    "            signals_per_cluster = np.array(signals_per_cluster)\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(signals_per_cluster) == 0:\n",
    "                number_of_lost_showers += 1               \n",
    "                continue\n",
    "            if len(signals_per_cluster) == 1:\n",
    "                second_to_first_ratio = 0.\n",
    "                second_to_first_ratios.append(second_to_first_ratio)\n",
    "            else:\n",
    "                second_to_first_ratio = np.sort(signals_per_cluster)[-2] / signals_per_cluster.max()\n",
    "                second_to_first_ratios.append(second_to_first_ratio)\n",
    "\n",
    "          \n",
    "\n",
    "            cluster = shower_data['clusters'][np.argmax(signals_per_cluster)]\n",
    "            \n",
    "            \n",
    "\n",
    "            # not enough signal\n",
    "            if (signals_per_cluster.max() / shower_data['numtracks']) <= 0.1:\n",
    "                continue\n",
    "\n",
    "            labels, counts = class_disbalance_graphx__(cluster)\n",
    "            counts = counts / counts.sum()\n",
    "            # high contamination\n",
    "            if counts[labels == shower_data['signal']] < 0.9:\n",
    "                number_of_stucked_showers += 1\n",
    "                #cluster = shower_data['clusters'][0]\n",
    "                e_stucked_10 = estimate_e(cluster, angle=0.1)\n",
    "                e_stucked_10_list.append(e_stucked_10)\n",
    "                \n",
    "                e_stucked_30 = estimate_e(cluster, angle=0.3)\n",
    "                e_stucked_30_list.append(e_stucked_30)\n",
    "                \n",
    "                e_stucked_50 = estimate_e(cluster, angle=0.5)\n",
    "                e_stucked_50_list.append(e_stucked_50)\n",
    "                \n",
    "                #print('stuck', shower_data['raw_clusters'])             \n",
    "                #stability_stucked.append(shower_data['raw_clusters'].stability)\n",
    "                continue\n",
    "\n",
    "            if second_to_first_ratio > 0.3:\n",
    "                number_of_broken_showers += 1\n",
    "                #cluster = shower_data['clusters'][0]\n",
    "                \n",
    "                e_broken_10 = estimate_e(cluster, angle=0.1)\n",
    "                e_broken_10_list.append(e_broken_10)\n",
    "                \n",
    "                e_broken_30 = estimate_e(cluster, angle=0.3)\n",
    "                e_broken_30_list.append(e_broken_30)\n",
    "                \n",
    "                e_broken_50 = estimate_e(cluster, angle=0.5)\n",
    "                e_broken_50_list.append(e_broken_50)\n",
    "                \n",
    "                #print('broken', shower_data['raw_clusters'])\n",
    "                #stability_broken.append(shower_data['raw_clusters'][0].stability)\n",
    "                continue\n",
    "                \n",
    "            \n",
    "\n",
    "            # for good showers\n",
    "            number_of_good_showers += 1\n",
    "\n",
    "            \n",
    "            # x, y, z\n",
    "            x, y, z = estimate_start_xyz(cluster)\n",
    "            \n",
    "            e_good_10 = estimate_e(cluster, angle=0.1)\n",
    "            e_good_30 = estimate_e(cluster, angle=0.3)\n",
    "            e_good_50 = estimate_e(cluster, angle=0.5)\n",
    "\n",
    "            e_good_10_list.append(e_good_10)\n",
    "            e_good_30_list.append(e_good_30)\n",
    "            e_good_50_list.append(e_good_50)\n",
    "            \n",
    "            #print('good', shower_data['raw_clusters'])\n",
    "            #stability_good.append(shower_data['raw_clusters'][0].stability)\n",
    "\n",
    "            x_raw.append(x)\n",
    "            x_true.append(shower_data['ele_SX'])\n",
    "\n",
    "            y_raw.append(y)\n",
    "            y_true.append(shower_data['ele_SY'])\n",
    "\n",
    "            z_raw.append(z)\n",
    "            z_true.append(shower_data['ele_SZ'])\n",
    "\n",
    "            # tx, ty\n",
    "            tx, ty = estimate_txty(cluster)\n",
    "\n",
    "            tx_raw.append(tx)\n",
    "            tx_true.append(shower_data['ele_TX'])\n",
    "\n",
    "            ty_raw.append(ty)\n",
    "            ty_true.append(shower_data['ele_TY'])\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_stucked_10_list = np.array(e_stucked_10_list)\n",
    "e_stucked_30_list = np.array(e_stucked_30_list)\n",
    "e_stucked_50_list = np.array(e_stucked_50_list)\n",
    "\n",
    "e_broken_10_list = np.array(e_broken_10_list)\n",
    "e_broken_30_list = np.array(e_broken_30_list)\n",
    "e_broken_50_list = np.array(e_broken_50_list)\n",
    "\n",
    "e_good_10_list = np.array(e_good_10_list)\n",
    "e_good_30_list = np.array(e_good_30_list)\n",
    "e_good_50_list = np.array(e_good_50_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_10 = np.hstack((e_stucked_10_list, e_broken_10_list, e_good_10_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.hstack((np.array([0]*len(e_stucked_10_list)), np.array([1]*len(e_broken_10_list)),\n",
    "                   np.array([2]*len(e_good_10_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)==len(e_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_30 = np.hstack((e_stucked_30_list, e_broken_30_list, e_good_30_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_50 = np.hstack((e_stucked_50_list, e_broken_50_list, e_good_50_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack((e_10,e_30,e_50)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12564, 3)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787798408488063"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, clf.predict(X_test), average = 'micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = showers[3].x.shape[1]\n",
    "edge_dim = showers[3].edge_features.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 32\n",
    "num_layers_emulsion = 5\n",
    "num_layers_edge_conv = 3\n",
    "min_samples_core = 4\n",
    "\n",
    "graph_embedder = GraphNN_KNN_v1(\n",
    "    output_dim=output_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    edge_dim=edge_dim,\n",
    "    num_layers_emulsion=num_layers_emulsion,\n",
    "    num_layers_edge_conv=num_layers_edge_conv,\n",
    "    input_dim=input_dim,\n",
    ").to(device)\n",
    "edge_classifier = EdgeClassifier_v1(\n",
    "    input_dim=2 * output_dim + edge_dim,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower = showers[3]\n",
    "\n",
    "G = preprocess_torch_shower_to_nx(\n",
    "    shower,\n",
    "    graph_embedder=graph_embedder,\n",
    "    edge_classifier=edge_classifier,\n",
    "    threshold=300,\n",
    "    add_noise=0.,\n",
    "    baseline=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components = []\n",
    "for cnn in nx.connected_components(nx.Graph(G)):\n",
    "    if len(cnn) > min_cl:\n",
    "        print(len(cnn), end=\", \")\n",
    "        connected_components.append(nx.DiGraph(G.subgraph(cnn)))\n",
    "connected_component = connected_components[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz linkage trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = nx.adjacency_matrix(nx.Graph(connected_component)).toarray().astype(np.float64)\n",
    "distance_matrix[distance_matrix == 0.] = np.inf\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=cl_size,\n",
    "                            min_samples=min_samples_core,\n",
    "                            metric=\"precomputed\",\n",
    "                            core_dist_n_jobs=-1)  # precomputed\n",
    "clusterer.fit(distance_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_hdbscan import run_hdbscan\n",
    "ewm_clusters, ewm_hdbscan, ewm_linkage = run_hdbscan(connected_component, cl_size=40)\n",
    "ewm_linkage = np.array(ewm_linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='paper', style=\"whitegrid\", font_scale=2)\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "clusterer.single_linkage_tree_.plot()\n",
    "#plt.savefig(\"hdbscan_single_linkage.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "hdbscan.plots.SingleLinkageTree(ewm_linkage).plot()\n",
    "#plt.savefig(\"ewm_single_linkage.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "clusterer.condensed_tree_.plot(select_clusters=True,\n",
    "                               selection_palette=sns.color_palette('deep', 8))\n",
    "#plt.savefig(\"hdbscan_condensed.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan._hdbscan_tree import condense_tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "hdbscan.plots.CondensedTree(condense_tree(ewm_linkage, 40)).plot(\n",
    "    select_clusters=True,\n",
    "    selection_palette=sns.color_palette('deep', 8)\n",
    ")\n",
    "#plt.savefig(\"ewn_condensed.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the parent denotes the id of the parent cluster, \n",
    "#the child the id of the child cluster (or, if the child is a single data point rather than a cluster, \n",
    "#the index in the dataset of that point), the lambda_val provides the lambda value at which the edge forms, \n",
    "#and the child_size provides the number of points in the child cluster. \n",
    "\n",
    "#As you can see the start of the DataFrame has singleton points falling out of the root cluster,\n",
    "#with each child_size equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan.plots.CondensedTree(condense_tree(ewm_linkage, 40)).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_clusters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz graph itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_COLORS = [\n",
    "    '#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#42d4f4', \n",
    "    'grey', '#469990', 'black', '#9A6324', '#fffac8', '#800000', \n",
    "    '#aaffc3', '#000075', '#a9a9a9', '#000000'\n",
    "]\n",
    "import copy\n",
    "from celluloid import Camera\n",
    "def plot_graphx(nodes: list, nodes_order: list, azim=-84, elev=10):\n",
    "    \"\"\"\n",
    "    Function for plotting shower\n",
    "    \"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "    import matplotlib.pyplot as plt\n",
    "    dZ = 205. / 10000.\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    camera = Camera(fig)\n",
    "    C = plt.cm.Blues(0.9)\n",
    "    ax = fig.gca(projection='3d')\n",
    "    # ax.set_xlabel(\"z\")\n",
    "    # ax.set_ylabel(\"y\")\n",
    "    # ax.set_zlabel(\"x\") \n",
    "    # ax.set_xlim(z0.min(), z1.max())\n",
    "    # ax.set_ylim(y0.min(), y1.max())\n",
    "    #  ax.set_zlim(x0.min(), x1.max())\n",
    "\n",
    "    x0, y0, z0 = [], [], []\n",
    "    sx, sy = [], []\n",
    "    colors = []\n",
    "    AVAILABLE_COLORS\n",
    "    signal = np.unique([n[1][\"signal\"] for n in nodes], return_counts=False)\n",
    "    print(signal)\n",
    "    map_signal_color = {}\n",
    "    for i, s in enumerate(signal):\n",
    "        map_signal_color[s] = AVAILABLE_COLORS[i]\n",
    "    for i, node_id in enumerate(nodes_order):\n",
    "        node = nodes[int(node_id)]\n",
    "        node = node[1]\n",
    "        if node[\"signal\"] == -1:\n",
    "            continue # skip noise nodes\n",
    "        x0.append(node['features']['SX'])\n",
    "        y0.append(node['features']['SY'])\n",
    "        z0.append(node['features']['SZ'])\n",
    "        sx.append(node['features']['TX'])\n",
    "        sy.append(node['features']['TY'])\n",
    "        colors.append(map_signal_color[node[\"signal\"]])\n",
    "        np.unique([n[1][\"signal\"] for n in nodes_new], return_counts=True)\n",
    "        if i % 10 == 0:\n",
    "            print(node_id)\n",
    "            x0_np, y0_np, z0_np = np.array(x0), np.array(y0), np.array(z0)\n",
    "            sx_np, sy_np = np.array(sx), np.array(sy)\n",
    "\n",
    "            x1_np = x0_np + dZ * sx_np\n",
    "            y1_np = y0_np + dZ * sy_np\n",
    "            z1_np = z0_np + dZ\n",
    "\n",
    "            start_points = np.array([z0_np, y0_np, x0_np]).T.reshape(-1, 3)\n",
    "            end_points = np.array([z1_np, y1_np, x1_np]).T.reshape(-1, 3)\n",
    "\n",
    "            lc = Line3DCollection(list(zip(start_points, end_points)), colors=colors, alpha=0.9, lw=2)\n",
    "            ax.set_xlim(z0_np.min(), z1_np.max())\n",
    "            ax.set_ylim(y0_np.min(), y1_np.max())\n",
    "            ax.set_zlim(x0_np.min(), x1_np.max())\n",
    "            ax.view_init(azim=azim, elev=elev)\n",
    "            ax.add_collection3d(lc)\n",
    "            camera.snap()\n",
    "    animation = camera.animate()\n",
    "    return animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_component_viz = copy.deepcopy(connected_component)\n",
    "nodes = list(connected_component_viz.nodes(data=True))\n",
    "edges = list(connected_component_viz.edges(data=True))\n",
    "\n",
    "nodes_added = []\n",
    "for q in ewm_linkage:\n",
    "    if (not q[0] in nodes_added) and (q[0] < len(connected_component)):\n",
    "        nodes_added.append(q[0])\n",
    "    if (not q[1] in nodes_added) and (q[1] < len(connected_component)):\n",
    "        nodes_added.append(q[1])\n",
    "        \n",
    "nodes_mapping = {}\n",
    "nodes_new = []\n",
    "for idx, node in enumerate(nodes):\n",
    "    nodes_mapping[node[0]] = idx\n",
    "    nodes_new.append((idx, node[1]))\n",
    "edges_new = []\n",
    "for idx, edge in enumerate(edges):\n",
    "    edges_new.append(\n",
    "        (nodes_mapping[edge[0]], nodes_mapping[edge[1]], edge[2])\n",
    "    )\n",
    "    \n",
    "animation = plot_graphx(nodes_new, nodes_added, azim=-30, elev=20)\n",
    "animation.save('ewm_clusterer.mp4', dpi=100,\n",
    "               savefig_kwargs={\n",
    "                   'frameon': False,\n",
    "                   'pad_inches': 'tight'\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "connected_component_viz = copy.deepcopy(connected_component)\n",
    "nodes = list(connected_component_viz.nodes(data=True))\n",
    "edges = list(connected_component_viz.edges(data=True))\n",
    "\n",
    "nodes_added = []\n",
    "for q in clusterer.single_linkage_tree_.to_numpy():\n",
    "    if (not q[0] in nodes_added) and (q[0] < len(connected_component)):\n",
    "        nodes_added.append(q[0])\n",
    "    if (not q[1] in nodes_added) and (q[1] < len(connected_component)):\n",
    "        nodes_added.append(q[1])\n",
    "        \n",
    "nodes_mapping = {}\n",
    "nodes_new = []\n",
    "for idx, node in enumerate(nodes):\n",
    "    nodes_mapping[node[0]] = idx\n",
    "    nodes_new.append((idx, node[1]))\n",
    "edges_new = []\n",
    "for idx, edge in enumerate(edges):\n",
    "    edges_new.append(\n",
    "        (nodes_mapping[edge[0]], nodes_mapping[edge[1]], edge[2])\n",
    "    )\n",
    "\n",
    "animation = plot_graphx(nodes_new, nodes_added, azim=-30, elev=20)\n",
    "animation.save('vanilla_clusterer.mp4', dpi=100,\n",
    "               savefig_kwargs={\n",
    "                   'frameon': False,\n",
    "                   'pad_inches': 'tight'\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_component_viz = copy.deepcopy(connected_component)\n",
    "nodes = list(connected_component_viz.nodes(data=True))\n",
    "edges = list(connected_component_viz.edges(data=True))\n",
    "\n",
    "nodes_added = []\n",
    "for q in ewm_linkage:\n",
    "    if (not q[0] in nodes_added) and (q[0] < len(connected_component)):\n",
    "        nodes_added.append(q[0])\n",
    "    if (not q[1] in nodes_added) and (q[1] < len(connected_component)):\n",
    "        nodes_added.append(q[1])\n",
    "        \n",
    "ewm_labels = np.full_like(clusterer.labels_, -1)\n",
    "for c, cluster in enumerate(ewm_clusters):\n",
    "    for node in cluster.nodes:\n",
    "        ewm_labels[nodes_mapping[node]] = c\n",
    "        \n",
    "nodes_mapping = {}\n",
    "nodes_new = []\n",
    "for idx, node in enumerate(nodes):\n",
    "    nodes_mapping[node[0]] = idx\n",
    "    nodes_new.append((idx, node[1]))\n",
    "    \n",
    "ewm_labels = np.full_like(clusterer.labels_, -1)\n",
    "for c, cluster in enumerate(ewm_clusters):\n",
    "    for node in cluster.nodes:\n",
    "        ewm_labels[nodes_mapping[node]] = c\n",
    "        \n",
    "for idx, node in enumerate(nodes_new):\n",
    "    node[1][\"signal\"] = ewm_labels[idx]\n",
    "    \n",
    "edges_new = []\n",
    "for idx, edge in enumerate(edges):\n",
    "    edges_new.append(\n",
    "        (nodes_mapping[edge[0]], nodes_mapping[edge[1]], edge[2])\n",
    "    )\n",
    "    \n",
    "    \n",
    "animation = plot_graphx(nodes_new, nodes_added, azim=-30, elev=20)\n",
    "animation.save('ewm_clusterer_with_labels.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_component_viz = copy.deepcopy(connected_component)\n",
    "nodes = list(connected_component_viz.nodes(data=True))\n",
    "edges = list(connected_component_viz.edges(data=True))\n",
    "\n",
    "nodes_added = []\n",
    "for q in clusterer.single_linkage_tree_.to_numpy():\n",
    "    if (not q[0] in nodes_added) and (q[0] < len(connected_component)):\n",
    "        nodes_added.append(q[0])\n",
    "    if (not q[1] in nodes_added) and (q[1] < len(connected_component)):\n",
    "        nodes_added.append(q[1])\n",
    "        \n",
    "nodes_mapping = {}\n",
    "nodes_new = []\n",
    "for idx, node in enumerate(nodes):\n",
    "    nodes_mapping[node[0]] = idx\n",
    "    nodes_new.append((idx, node[1]))\n",
    "    node[1][\"signal\"] = clusterer.labels_[idx]\n",
    "    \n",
    "edges_new = []\n",
    "for idx, edge in enumerate(edges):\n",
    "    edges_new.append(\n",
    "        (nodes_mapping[edge[0]], nodes_mapping[edge[1]], edge[2])\n",
    "    )\n",
    "\n",
    "animation = plot_graphx(nodes_new, nodes_added, azim=-30, elev=20)\n",
    "animation.save('vanilla_clusterer_with_labels.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
