{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Sequential\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch_geometric.transforms as T\n",
    "import torch_cluster\n",
    "\n",
    "from torch_geometric.nn import NNConv, GCNConv, GraphConv\n",
    "from torch_geometric.nn import PointConv, EdgeConv, SplineConv\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from torch_geometric.data import DataLoader   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade --force-reinstall torch-sparse && pip install --upgrade --force-reinstall torch-cluster && pip install --upgrade --force-reinstall torch-scatter && pip install --upgrade --force-reinstall torch-spline_conv && pip install --upgrade --force-reinstall torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_train_10_F = torch.load('./EM_data/train_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(showers_train_10_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_train = [shower.to(device) for shower in showers_train_10_F[:120]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(showers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_train[0].x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(showers_train)):\n",
    "    data = showers_train[i].x\n",
    "    \n",
    "    shape_0 = data.shape[0]\n",
    "    \n",
    "    #azimuthal_angle\n",
    "    feat_0 = torch.atan(data[:, 1]/(data[:, 0]+0.00001)).view(shape_0, 1)\n",
    "    \n",
    "    feat_1 = (torch.sqrt(torch.pow(data[:, 1], 2) + torch.pow(data[:, 0],2))\n",
    "              /(data[:, 2]+0.00001)).view(shape_0,1)\n",
    "    \n",
    "    feat_2 = (data[:, 0]/(data[:, 2]+0.00001)).view(shape_0,1)\n",
    "    \n",
    "    feat_3 = (data[:, 1]/(data[:, 2]+0.00001)).view(shape_0,1)\n",
    "    \n",
    "    feat_4 = (torch.sin(feat_0) + torch.cos(feat_0))/(feat_0+0.00001)\n",
    "    \n",
    "    showers_train[i].x = torch.cat((data, feat_0, feat_1, feat_2, feat_3, feat_4), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_train[0].x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking weather edges connect hits from the same shower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare Z-coordinates of each brick layer with our data Z-coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([     0.,   1293.,   2586.,   3879.,   5172.,   6465.,   7758.,\n",
    "                          9051.,  10344.,  11637.,  12930.,  14223.,  15516.,  16809.,\n",
    "                         18102.,  19395.,  20688.,  21981.,  23274.,  24567.,  25860.,\n",
    "                         27153.,  28446.,  29739.,  31032.,  32325.,  33618.,  34911.,\n",
    "                         36204.,  37497.,  38790.,  40083.,  41376.,  42669.,  43962.,\n",
    "                         45255.,  46548.,  47841.,  49134.,  50427.,  51720.,  53013.,\n",
    "                         54306.,  55599.,  56892.,  58185.,  59478.,  60771.,  62064.,\n",
    "                         63357.,  64650.,  65943.,  67236.,  68529.,  69822.,  71115.,\n",
    "                         72408.,  73701.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_centered = Z - 73701. / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_0 = np.array(showers_train[0].x[:, 2].cpu())*np.array([1e4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# the histogram of the data\n",
    "plt.hist(Z_0, 100, density=True)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# the histogram of the data\n",
    "plt.hist(Z_centered, 100, density=True)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks for activating only those edges that are connecting the same shower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_Z_coodr(x):\n",
    "        return Z_centered[np.argmin(np.abs(Z_centered - x))]\n",
    "round_Z_coodr = np.vectorize(round_Z_coodr)\n",
    "    \n",
    "def mask(data):\n",
    "    x = data.x[:, 2]\n",
    "    \n",
    "    Z_0 = np.array(x.cpu()) * np.array([1e4])\n",
    "    \n",
    "    z_rounded = round_Z_coodr(Z_0)\n",
    "    \n",
    "    orders = np.zeros((len(Z_centered), data.edge_index.shape[1]))\n",
    "\n",
    "    for i, z_i in enumerate(Z_centered):\n",
    "        for j, z_0_i in enumerate(z_rounded):\n",
    "            if z_0_i == z_i:\n",
    "                # TODO data.edge_index[0] vs data.edge_index[1]\n",
    "                orders[i][(data.edge_index[0] == j).cpu().numpy().astype(bool)] = 1\n",
    "            \n",
    "    return orders.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "for i in range(len(showers_train)):\n",
    "    masks.append(mask(showers_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmulsionConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')\n",
    "        self.mp = torch.nn.Linear(in_channels * 2, out_channels).to(device)\n",
    "        \n",
    "    def forward(self, x, edge_index, orders):\n",
    "        for order in orders: \n",
    "            x = self.propagate(torch.index_select(edge_index[:, torch.ByteTensor(order).to(device)], \n",
    "                                                  0, \n",
    "                                                  torch.LongTensor([1, 0]).to(device)), x=x)     \n",
    "            \n",
    "        return x\n",
    "\n",
    "    def message(self, x_j, x_i):\n",
    "        return self.mp(torch.cat([x_i, x_j - x_i], dim=1))\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        return aggr_out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = showers_train[0].x.shape[1]\n",
    "\n",
    "class GraphNN_KNN(torch.nn.Module):\n",
    "    n = showers_train[0].x.shape[1]\n",
    "    \n",
    "    def __init__(self, k=n, dim_out=10):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        \n",
    "        self.emconv = EmulsionConv(self.k, self.k)    \n",
    "      \n",
    "\n",
    "        self.wconv1 = EdgeConv(Sequential(nn.Linear(20, 10)), 'max')\n",
    "        self.wconv2 = EdgeConv(Sequential(nn.Linear(20, 10)), 'max')\n",
    "        self.wconv3 = EdgeConv(Sequential(nn.Linear(20, 10)), 'max')\n",
    "\n",
    "   \n",
    "        self.output = nn.Linear(10, dim_out)      \n",
    "\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, orders = data.x, data.edge_index, data.mask\n",
    "        \n",
    "        x = self.emconv(x=x, edge_index=edge_index, orders=orders)  \n",
    "        \n",
    "                         \n",
    "        x1 = self.wconv1(x=x, edge_index=edge_index)\n",
    "        x2 = self.wconv2(x=x1, edge_index=edge_index)\n",
    "        x3 = self.wconv3(x=x2, edge_index=edge_index)\n",
    "\n",
    "        \n",
    "        return self.output(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphNN_KNN(dim_out=288).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(showers_train)):\n",
    "    showers_train[i].mask = torch.tensor(masks[i], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(showers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=1, shuffle=False)\n",
    "\n",
    "#valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    #sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "edge_classifier = nn.Sequential(nn.Linear(288*2, 144),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(144, 144),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(144, 32),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(32, 1),\n",
    "                                nn.Sigmoid()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(edge_classifier.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "accs = []\n",
    "\n",
    "aucs = []\n",
    "    \n",
    "for i in range(1, n_epoch + 1):\n",
    "      \n",
    "    for shower in train_loader:\n",
    "\n",
    "        embeddings = model(shower)\n",
    "\n",
    "\n",
    "        edge_labels = (shower.y[shower.edge_index[0]] \n",
    "                       == shower.y[shower.edge_index[1]])  \n",
    "\n",
    "\n",
    "\n",
    "        edge_data = torch.cat([embeddings[shower.edge_index[0]], \n",
    "                               embeddings[shower.edge_index[1]]], dim=1)\n",
    "\n",
    "\n",
    "        edge_predicted = edge_classifier(edge_data)\n",
    "\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(edge_predicted.view(-1), edge_labels.view(-1).float())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "\n",
    "    for shower in test_loader:\n",
    "        \n",
    "        embeddings = model(shower)\n",
    "\n",
    "\n",
    "        edge_labels = (shower.y[shower.edge_index[0]] \n",
    "                           == shower.y[shower.edge_index[1]])  \n",
    "\n",
    "\n",
    "\n",
    "        edge_data = torch.cat([embeddings[shower.edge_index[0]], \n",
    "                                   embeddings[shower.edge_index[1]]], dim=1)\n",
    "\n",
    "\n",
    "        edge_predicted = edge_classifier(edge_data)\n",
    "\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(edge_predicted.view(-1), edge_labels.view(-1).float())\n",
    "        loss.backward()\n",
    "        test_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        y_true = edge_labels.view(-1).float().cpu().detach().numpy()\n",
    "        y_pred = edge_predicted.view(-1).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred.round())\n",
    "\n",
    "        accs.append(acc)\n",
    "\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        \n",
    "        aucs.append(auc)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "        \n",
    "        average_precision = average_precision_score(y_true, y_pred)\n",
    "\n",
    "        clear_output()\n",
    "        \n",
    "        print(average_precision)\n",
    "        \n",
    "        \n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, sharex=False, figsize=(12,5))\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "        ax = axs[0]\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--')\n",
    "        # plot the roc curve for the model\n",
    "        ax.plot(fpr, tpr, marker='.')\n",
    "        ax.set_title('ROC curve', fontsize=15)\n",
    "        ax.set_xlabel(\"False Positive Rate (FPR)\", fontsize=13)\n",
    "        ax.set_ylabel(\"True Positive Rate (TPR)\", fontsize=13)\n",
    "\n",
    "        ax = axs[1]\n",
    "        ax.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "        ax.plot(recall, precision, marker='.')\n",
    "        ax.set_title('Precision-Recall Curve', fontsize=15)\n",
    "        ax.set_xlabel(\"Precision\", fontsize=13)\n",
    "        ax.set_ylabel(\"Recall\", fontsize=13)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"#%i, Test loss: %.7f\"%(i+1,np.mean(test_losses)),flush=True)\n",
    "    print('mean AUC per batch: %.3f' % np.mean(aucs))\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, sharex=False, figsize=(16,10))\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.plot(test_losses)\n",
    "    ax.set_title('Loss')\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.plot(accs)\n",
    "    ax.set_title('Accuracy')\n",
    "\n",
    "\n",
    "\n",
    "    fig.suptitle('Test set metrics')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_edge_classifier.sav'\n",
    "loaded_edge_classifier = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for shower in test_loader:\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    print(i)\n",
    "        \n",
    "    embeddings = loaded_model(shower)\n",
    "\n",
    "    edge_labels = (shower.y[shower.edge_index[0]] \n",
    "                           == shower.y[shower.edge_index[1]])  \n",
    "\n",
    "    edge_data = torch.cat([embeddings[shower.edge_index[0]], \n",
    "                                   embeddings[shower.edge_index[1]]], dim=1)\n",
    "\n",
    "    edge_predicted = loaded_edge_classifier(edge_data)\n",
    "    \n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showers_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWER = showers_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWER.weight = torch.tensor(edge_predicted, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOWER.is_directed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(SHOWER.y.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clear the edges with predicted weight < 0.45*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = SHOWER.edge_index.t()[(edge_predicted > 0.5).view(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Collect weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = SHOWER.weight[(edge_predicted > 0.65).view(-1)].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clear single points*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.unique(edge_indices.view(-1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check the direction of th graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SX = SHOWER.pos.t()[0].cpu().detach().numpy()\n",
    "SY = SHOWER.pos.t()[1].cpu().detach().numpy()\n",
    "SZ = SHOWER.pos.t()[2].cpu().detach().numpy()\n",
    "TX = SHOWER.pos.t()[3].cpu().detach().numpy()\n",
    "TY = SHOWER.pos.t()[4].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtracks = SHOWER.shower_data.t()[0].cpu().detach().numpy()\n",
    "ele_P = SHOWER.shower_data.t()[1].cpu().detach().numpy()\n",
    "ele_SX = SHOWER.shower_data.t()[2].cpu().detach().numpy()\n",
    "ele_SY = SHOWER.shower_data.t()[3].cpu().detach().numpy()\n",
    "ele_SZ = SHOWER.shower_data.t()[4].cpu().detach().numpy()\n",
    "ele_TX = SHOWER.shower_data.t()[5].cpu().detach().numpy()\n",
    "ele_TY = SHOWER.shower_data.t()[6].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Directed Graph Creating*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add nodes to graph\n",
    "nodes_to_add = deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SHOWERS_IN_BRICK = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(NUM_SHOWERS_IN_BRICK):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in points:\n",
    "    data_to_add = (\n",
    "            (\n",
    "                k,\n",
    "                {  \n",
    "                    'features': {\n",
    "                    \n",
    "                        'SX': SX[k]*5000,\n",
    "                        'SY': SY[k]*5000,\n",
    "                        'SZ': SZ[k]*5000,\n",
    "                        'TX': TX[k],\n",
    "                        'TY': TY[k],\n",
    "                    },\n",
    "                    \n",
    "                    \n",
    "                ####need to be changed!\n",
    "                    'signal': k,\n",
    "                    \n",
    "                    'shower_data': {\n",
    "                    \n",
    "                        'numtracks': numtracks[k],\n",
    "                        'ele_P': ele_P[k],\n",
    "                        'ele_SX': ele_SX[k],\n",
    "                        'ele_SY': ele_SY[k],\n",
    "                        'ele_SZ': ele_SZ[k],\n",
    "                        'ele_TX': ele_TX[k],\n",
    "                        'ele_TY': ele_TY[k]\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    nodes_to_add.append(data_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(nodes_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices_array =  edge_indices.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "edges_to_add = deque()\n",
    "for k in range(len(weights)):\n",
    "    p0 = edge_indices_array[k][0]\n",
    "    p1 = edge_indices_array[k][1]\n",
    "    weight = 1.0-float(weights[k])\n",
    "    edges_to_add.append((p0, p1, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_weighted_edges_from(edges_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering\n",
    "\n",
    "@total_ordering\n",
    "class ClusterHDBSCAN(object):\n",
    "    def __init__(self, weight: float, cl_size: int, clusters: list = None, nodes: list = None):\n",
    "        # init\n",
    "        self.nodes = set()\n",
    "        self.nodes_in = Counter()\n",
    "        self.nodes_out = Counter()\n",
    "        \n",
    "        self.weights_nodes_dict = defaultdict(set)\n",
    "        \n",
    "        self.weight_death = weight\n",
    "        self.lambda_death = 1. / (weight + 1e-5)\n",
    "        \n",
    "        self.weight_birth = weight\n",
    "        self.lambda_birth = 1. / (weight + 1e-5)\n",
    "        \n",
    "        \n",
    "        self.children = []\n",
    "        self.falling_out_points = []\n",
    "        \n",
    "        assert clusters is not None or nodes is not None\n",
    "        if clusters is not None:\n",
    "            for cluster in clusters:\n",
    "                self.nodes.update(cluster.nodes)\n",
    "                self.nodes_in.update(cluster.nodes_in)\n",
    "                self.nodes_out.update(cluster.nodes_out)\n",
    "                self.weights_nodes_dict[weight].update(cluster.nodes)\n",
    "                if cluster.is_cluster:\n",
    "                    cluster.set_weight_birth(weight)\n",
    "                    self.children.append(cluster)\n",
    "                else:\n",
    "                    self.falling_out_points.append(cluster)\n",
    "        else:\n",
    "            self.nodes.update(nodes)\n",
    "            self.nodes_out.update(nodes)\n",
    "            self.weights_nodes_dict[weight].update(nodes)\n",
    "        self.frozennodes = frozenset(self.nodes)\n",
    "        self.__hash = hash(self.frozennodes)\n",
    "        self.listnodes = list(self.nodes)\n",
    "        self.npnodes = np.array(list(self.nodes)).astype(np.int32)\n",
    "        self.cl_size = cl_size\n",
    "        self.is_cluster = len(self) >= cl_size\n",
    "        self.is_noise = not self.is_cluster\n",
    "        self.stability = None\n",
    "        \n",
    "        \n",
    "    def append(self, weight: float, clusters: list):\n",
    "        \"\"\"\n",
    "        Adding\n",
    "        \"\"\"\n",
    "        for cluster in clusters:\n",
    "            self.nodes.update(cluster.nodes)\n",
    "            self.weights_nodes_dict[weight].update(cluster.nodes)\n",
    "        self.weight_birth = weight\n",
    "        self.lambda_birth = 1 / (weight + 1e-5)\n",
    "        self.frozennodes = frozenset(self.nodes)\n",
    "        self.__hash = hash(self.frozennodes)\n",
    "        self.listnodes = list(self.nodes)\n",
    "        self.npnodes = np.array(list(self.nodes)).astype(np.int32)\n",
    "        self.is_cluster = len(self) >= self.cl_size\n",
    "        self.is_noise = not self.is_cluster\n",
    "        return self\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for child in self.children:\n",
    "            yield child\n",
    "    \n",
    "    def __contains__(self, node):\n",
    "        return node in self.nodes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nodes)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return self.__hash\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.__hash == other.__hash\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.__hash < other.__hash\n",
    "    \n",
    "    def set_weight_birth(self, weight: float):\n",
    "        self.weight_birth = weight\n",
    "        self.lambda_birth = 1 / (weight + 1e-5)\n",
    "        \n",
    "    def calculate_stability(self):\n",
    "        self.stability = 0.\n",
    "        self.lambda_birth = 1 / (max(self.weights_nodes_dict.keys()) + 1e-5)\n",
    "        norm = self.lambda_birth\n",
    "        for weight in self.weights_nodes_dict:\n",
    "            self.stability += len(self.weights_nodes_dict[weight]) * (1 / (weight + 1e-5) - self.lambda_birth) * norm\n",
    "            \n",
    "def calc_stabilities(root):\n",
    "    root.calculate_stability()\n",
    "    for child in root:\n",
    "        calc_stabilities(child)\n",
    "        \n",
    "        \n",
    "def class_disbalance(cluster, graph):\n",
    "    subgraph = graph.subgraph(cluster.nodes)\n",
    "    signal = []\n",
    "    for _, node in subgraph.nodes(data=True):\n",
    "        signal.append(node['signal'])\n",
    "    return list(zip(*np.unique(signal, return_counts=True)))\n",
    "\n",
    "def flat_clusters(root):\n",
    "    if root.is_cluster:\n",
    "        yield root\n",
    "        \n",
    "    for child in root:\n",
    "        for cluster in flat_clusters(child):\n",
    "            yield cluster\n",
    "            \n",
    "def reed_stabilities(root, level=0):\n",
    "    print('    ' * (level - 1) + '+---' * (level > 0), end='')\n",
    "    print('len={}'.format(len(root)), end=' ')\n",
    "    print('stability={:.2f}'.format(root.stability))\n",
    "    for child in root:\n",
    "        reed_stabilities(child, level + 1)\n",
    "\n",
    "def print_class_disbalance_for_all_clusters(root, graph, level=0):\n",
    "    class_disbalance_tuples = class_disbalance(root, graph)\n",
    "\n",
    "    print('    ' * (level - 1) + '+---' * (level > 0), end='')\n",
    "    print('len={}'.format(len(root)))\n",
    "    print('    ' * (level), end='')\n",
    "    print(class_disbalance_tuples, end=' ')\n",
    "    print('stability={:.3f}'.format(root.stability))\n",
    "    for child in root:\n",
    "        print_class_disbalance_for_all_clusters(child, graph, level + 1)\n",
    "        \n",
    "def leaf_clusters(root):\n",
    "    if root.is_cluster and len(root.children) == 0:\n",
    "        yield root\n",
    "        \n",
    "    for child in root:\n",
    "        for cluster in leaf_clusters(child):\n",
    "            yield cluster\n",
    "\n",
    "def max_level_clusters(root, level=0, max_level=2):\n",
    "    if level == max_level and root.is_cluster:\n",
    "        yield root\n",
    "        \n",
    "    for child in root:\n",
    "        for cluster in max_level_clusters(child, level=level+1, max_level=max_level):\n",
    "            yield cluster\n",
    "            \n",
    "            \n",
    "def recalc_tree(root):\n",
    "    weights_children = 0\n",
    "    for child in root:\n",
    "        weights_children += recalc_tree(child)\n",
    "    if weights_children > root.stability:\n",
    "        root.stability = weights_children\n",
    "    else:\n",
    "        root.children.clear()\n",
    "    \n",
    "    return root.stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "def run_hdbscan(G, cl_size=20, order=True):\n",
    "    ####### core_d was deleted => could be returned. Laverage robustness / cluster shaprness.\n",
    "    edges = []\n",
    "    for node_id_left, node_id_right, edge in G.edges(data=True):\n",
    "        node_left = G.node[node_id_left]\n",
    "        node_right = G.node[node_id_right]\n",
    "        edges.append(\n",
    "            (\n",
    "            node_id_left, node_id_right, edge['weight'], np.sign(node_left['features']['SZ'] - node_right['features']['SZ'])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #nx.mini\n",
    "    #edges = list(G.edges(data=True))\n",
    "    #edges = [(edge[0], # 0\n",
    "    #          edge[1], # 1\n",
    "    #          edge[2]['weight'], # 2\n",
    "    #         ) for edge in edges]\n",
    "    \n",
    "    edges = sorted(edges, key = operator.itemgetter(2))\n",
    "\n",
    "    ### Minimum spanning tree was also thrown\n",
    "    ### following algo reminds of Kruskal algo but with some modifications\n",
    "\n",
    "    ### TODO: sort on some mix(i.e. linear) of Z_coord and integrated distance\n",
    "    \n",
    "    # init\n",
    "    clusters = {}\n",
    "    for node_id in G.nodes():\n",
    "        clusters[node_id]=ClusterHDBSCAN(cl_size=cl_size, weight=np.inf, nodes=[node_id])\n",
    "\n",
    "\n",
    "    for i, j, weight, *_ in edges:\n",
    "        cluster_out = clusters[i]\n",
    "        cluster_in = clusters[j]\n",
    "\n",
    "        ### tunable parameter\n",
    "        #if cluster_in.nodes_in[j] > 1:\n",
    "        #    continue\n",
    "\n",
    "        if cluster_in is cluster_out:\n",
    "            continue\n",
    "\n",
    "        if cluster_in.is_cluster and cluster_out.is_cluster:\n",
    "            cluster = ClusterHDBSCAN(weight=weight , cl_size=cl_size, clusters=[cluster_in, cluster_out])\n",
    "        elif cluster_in.is_cluster and not cluster_out.is_cluster:\n",
    "            cluster = cluster_in.append(weight=weight, clusters=[cluster_out])\n",
    "        elif cluster_out.is_cluster and not cluster_in.is_cluster:\n",
    "            cluster = cluster_out.append(weight=weight, clusters=[cluster_in])\n",
    "        else:\n",
    "            cluster = ClusterHDBSCAN(weight=weight, cl_size=cl_size, clusters=[cluster_in, cluster_out])\n",
    "\n",
    "        cluster.nodes_out[i] += 1\n",
    "        cluster.nodes_in[j] += 1\n",
    "\n",
    "        clusters.update({l: cluster for l in cluster.nodes})\n",
    "        \n",
    "    clusters = list(set(clusters.values()))\n",
    "\n",
    "    ### choose biggest cluster\n",
    "    root = clusters[0]\n",
    "    length = len(clusters[0])\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) > length:\n",
    "            length = len(cluster)\n",
    "            root = cluster\n",
    "    \n",
    "    calc_stabilities(root)\n",
    "    #recalc_tree(root)\n",
    "    clusters = list(leaf_clusters(root))\n",
    "    return clusters, root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hdbscan_on_brick(graphx, min_cl=60, cl_size=60, order=True):\n",
    "    connected_components = []\n",
    "    for cnn in nx.connected_components(nx.Graph(graphx)):\n",
    "        if len(cnn) > min_cl:\n",
    "            connected_components.append(nx.DiGraph(graphx.subgraph(cnn)))\n",
    "    clusters = []\n",
    "    roots = []\n",
    "    for G in connected_components:\n",
    "        if len(G) < 100:\n",
    "            clusters.append(G)\n",
    "        else:\n",
    "            clusters_hdbscan, root_hdbscan = run_hdbscan(G, cl_size=cl_size, order=order)\n",
    "            roots.append(root_hdbscan)\n",
    "            clusters.extend(clusters_hdbscan)\n",
    "    \n",
    "    return graphx, clusters, roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphx, clusters, roots = run_hdbscan_on_brick(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.opera_tools import plot_graphx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphx(graphx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = graphx.subgraph(clusters[2].nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = []\n",
    "for _, node in subgraph.nodes(data=True):\n",
    "    signal.append(node['signal'])\n",
    "list(zip(*np.unique(signal, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(clusters)):\n",
    "    plot_graphx(graphx.subgraph(clusters[i].nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the quality of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def estimate_e(cluster, angle=0.05):\n",
    "    x, y, z = estimate_start_xyz(cluster)\n",
    "    tx, ty = estimate_txty(cluster)\n",
    "    n = 0\n",
    "    for i, node in cluster.nodes(data=True):\n",
    "        dx = node['features']['SX'] - x\n",
    "        dy = node['features']['SY'] - y\n",
    "        dz = node['features']['SZ'] - z \n",
    "        dx = dx / dz - tx\n",
    "        dy = dy / dz - ty\n",
    "        dz = dz / dz\n",
    "        if sqrt(dx**2 + dy**2) < angle:\n",
    "            n += 1\n",
    "            \n",
    "    return n\n",
    "\n",
    "def estimate_start_xyz(cluster, k=3, shift_x=0., shift_y=0., shift_z=-2000.):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    zs = []\n",
    "\n",
    "    for i, node in cluster.nodes(data=True):\n",
    "        xs.append(node['features']['SX'])\n",
    "        ys.append(node['features']['SY'])\n",
    "        zs.append(node['features']['SZ'])\n",
    "    \n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    zs = np.array(zs)\n",
    "    \n",
    "    argosorted_z = np.argsort(zs)\n",
    "    \n",
    "    x = np.median(np.median(xs[argosorted_z][:k])) + shift_x\n",
    "    y = np.median(np.median(ys[argosorted_z][:k])) + shift_y\n",
    "    z = np.median(np.median(zs[argosorted_z][:k])) + shift_z\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "def estimate_txty(cluster, k=20):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    zs = []\n",
    "    tx = []\n",
    "    ty = []\n",
    "\n",
    "    for i, node in cluster.nodes(data=True):\n",
    "        xs.append(node['features']['SX'])\n",
    "        ys.append(node['features']['SY'])\n",
    "        zs.append(node['features']['SZ'])\n",
    "        tx.append(node['features']['TX'])\n",
    "        ty.append(node['features']['TY'])\n",
    "        \n",
    "    \n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    zs = np.array(zs)\n",
    "    tx = np.array(tx)\n",
    "    ty = np.array(ty)\n",
    "    \n",
    "    \n",
    "    argosorted_z = np.argsort(zs)\n",
    "    \n",
    "    lr = TheilSenRegressor()\n",
    "    lr.fit(zs[argosorted_z][:k].reshape((-1, 1)), xs[argosorted_z][:k])\n",
    "    TX = lr.coef_[0]\n",
    "\n",
    "    lr.fit(zs[argosorted_z][:k].reshape((-1, 1)), ys[argosorted_z][:k])\n",
    "    TY = lr.coef_[0]\n",
    "    \n",
    "    return TX, TY\n",
    "    return np.median(np.median(tx[argosorted_z][:k])), np.median(np.median(ty[argosorted_z][:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tracks = 0\n",
    "total_tracks = 0\n",
    "\n",
    "number_of_lost_showers = 0\n",
    "number_of_broken_showers = 0\n",
    "number_of_stucked_showers = 0\n",
    "total_number_of_showers = 0\n",
    "number_of_good_showers = 0\n",
    "number_of_survived_showers = 0\n",
    "second_to_first_ratios = []\n",
    "\n",
    "E_raw = []\n",
    "E_true = []\n",
    "\n",
    "x_raw = []\n",
    "x_true = []\n",
    "\n",
    "y_raw = []\n",
    "y_true = []\n",
    "\n",
    "z_raw = []\n",
    "z_true = []\n",
    "\n",
    "tx_raw = []\n",
    "tx_true = []\n",
    "\n",
    "ty_raw = []\n",
    "ty_true = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in clusters:\n",
    "    selected_tracks += len(cluster)\n",
    "    for label, label_count in class_disbalance(cluster, graphx):\n",
    "        print(label, label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why all label_counts == 1?\n",
    "#is the problem in adding 'signal' to our graph?\n",
    "# <- why we have to iterate j on len(selected_showers)?\n",
    "# <- selected_showers are received using pmc_to_ship_format function\n",
    "# <- where is the file '../data/mcdata_taue2.root'\n",
    "\n",
    "#https://gitlab.com/SchattenGenie/wunder-nmp/blob/pytorch/ship/clusterization_with_hdbscan_ship_data_run_200.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = graphx.subgraph(clusters[0].nodes)\n",
    "signal = []\n",
    "for _, node in subgraph.nodes(data=True):\n",
    "    signal.append(node['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
